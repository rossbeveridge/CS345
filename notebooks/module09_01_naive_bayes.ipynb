{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is part of  course materials for CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
    "Original versions were created by Asa Ben-Hur.\n",
    "The content is availabe [on GitHub](https://github.com/asabenhur/CS345).*\n",
    "\n",
    "*The text is released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/), and code is released under the [MIT license](https://opensource.org/licenses/MIT).*\n",
    "\n",
    "<img style=\"padding: 10px; float:right;\" alt=\"CC-BY-SA icon.svg in public domain\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/d0/CC-BY-SA_icon.svg\" width=\"125\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github//asabenhur/CS345/blob/master/notebooks/module09_01_naive_bayes.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%autosave 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Probability In Learning\n",
    "\n",
    "Given an estimate of $p(y | \\mathbf{x})$, we would make a prediction according to \n",
    "\n",
    "$$\\hat{y} = \\mathrm{arg} \\max_y p(y | \\mathbf{x}).$$ \n",
    "\n",
    "In logistic regression we estimated $p(y | \\mathbf{x})$ directly.  Here we will take a different route, using Bayes' rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' rule\n",
    "\n",
    "The following relationship between two variables is known as *Bayes' rule*:\n",
    "\n",
    "$$\n",
    "    p(y|x)=\\frac{p(x|y)p(y)}{p(x)}\n",
    "$$\n",
    "\n",
    "In machine learning we use an extended version of Bayes rule that describes the relationship between a label $y$ and a vector of variables $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "    p(y|\\mathbf{x})=\\frac{p(\\mathbf{x}|y)p(y)}{p(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "The Naive Bayes classifier is built on the basis of this relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' rule as a mode of inference\n",
    "\n",
    "From the perspective of machine learning, Bayes rule provides a way of updating our confidence in an outcome as more evidence comes along.\n",
    "Our *prior* probability for a class $y$ is $p(y)$.  After making an observation $\\mathbf{x}$, we need to update our belief to $p(y|\\mathbf{x})$ which is called the *posterior* probability.\n",
    "The relationship between the posterior and prior is given by Bayes rule:\n",
    "\n",
    "$$\n",
    "\\underbrace{p(y|\\mathbf{x})}_{\\mathrm{posterior}}=\\frac{\\overbrace{p(\\mathbf{x}|y)}^{\\mathrm{likelihood}}\\overbrace{p(y)}^{\\mathrm{prior}}}{p(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Or in other words:\n",
    "\n",
    "$$\\mathrm{posterior} \\propto \\mathrm{likelihood} \\times \\mathrm{prior} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum A-posteriori\n",
    "\n",
    "The maximum a posteriori (MAP) rule:\n",
    "\n",
    "$$\n",
    "y_{\\mathrm{MAP}}=\\mathrm{arg}\\max_y p(y|\\mathbf{x}) = \\mathrm{arg}\\max_y \\frac{p(\\mathbf{x}|y)p(y)}{p(\\mathbf{x})}=\\mathrm{arg}\\max_y p(\\mathbf{x}|y)p(y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Bayes’ rule \n",
    "\n",
    "To use the MAP estimator we will estimate $p(\\mathbf{x}|y)$.\n",
    "\n",
    "**Example**: Predict whether or not a picnic spot is enjoyable \n",
    "\n",
    "Assume that we have the following table as training data that relate a collection of features (whether it's sunny/rainy the temperature is warm/cold, the humidity, wind, and forecast) to the label that describes our level of enjoyment from our picnic spot:\n",
    "\n",
    "|  |                 |                |                |                |     |     |\n",
    "|--------------|--------|---------|----------------|----------------|------------------------|-------|\n",
    "|              |$X_1$|$X_2$|$X_3$|$X_4$|$X_5$|$X_6$|$Y$|\n",
    "|              |Sky|Temp|Humidity|Wind|Near water|EnjoySpt|\n",
    "| **Row 1** |Sunny|Warm|Normal|Strong|Yes|Yes|\n",
    "| **Row 2** |Sunny|Warm|High|Strong|No|Yes|\n",
    "| **Row 3** |Rainy|Cold|High|Weak|No|No|\n",
    "| **Row 4** |Cloudy|Mild|High|Weak|No|Yes|\n",
    "\n",
    "\n",
    "\n",
    "How many parameters does this model have?\n",
    "We will assume discrete binary features and a binary classification problem.\n",
    "\n",
    "* Prior $p(y)$:  1 parameter\n",
    "* Likelihood $p(\\mathbf{x} | y)$: $2(2^d – 1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "* What is the number of parameters for a classification problem with $k$ classes?\n",
    "* What if each feature has $v$ possible values instead of two?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Naive Bayes assumption\n",
    "\n",
    "Clearly, modeling $p(\\mathbf{x} | y)$ directly does not scale well with the number of features.  We would need an exponential number of examples to reliably estimate those parameters (exponential in the number of features).\n",
    "\n",
    "We will use the simplifying assumption of conditional independence:  given the class label, we will assume that the features are independent, i.e.\n",
    "$$\n",
    "p(\\mathbf{x}|y)=p(x_1|y)p(x_2|y),\\ldots,p(x_d|y).\n",
    "$$\n",
    "\n",
    "\n",
    "This model is now tractable and has $2d+1$ parameters for a binary classification problem with binary features.\n",
    "\n",
    "We can now use Bayes' rule to obtain the Naive Bayes decision rule:\n",
    "$$\n",
    "y_{NB}=\\mathrm{arg} \\max_y P(\\mathbf{x}|y)P(y)= \\mathrm{arg} \\max_y \\prod_{i=1}^{d} p(x_i|y)p(y)\n",
    "$$\n",
    "\n",
    "If conditional independence holds, Naive Bayes is an optimal classifier!\n",
    "Our next task is to estimate $p(x_i|y)$, which we will do using the principle of maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial naive Bayes\n",
    "\n",
    "Naive Bayes was one of the standard methods for text classification.  Binomial naive Bayes is motivated by this application.  For text classification we can represent a document by a vector of 1s and 0s that indicate presence/absence of a given word in the document.\n",
    "\n",
    "Let's demonstrate that using scikit-learn's `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 1 1 0 1]\n",
      " [1 0 0 1 0 1 0 1 1 1]\n",
      " [0 1 1 1 1 0 0 1 0 1]]\n",
      "['and', 'document', 'first', 'is', 'not', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'This is not the first document']\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the return value of `CountVectorizer` is not a standard array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection:\n",
    "\n",
    "What are we missing by converting a document to a vector of presence/absence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Naive Bayes\n",
    "\n",
    "Under this simple assumption that a feature occurs/doesn't occur, a very simple probabilistic model will work - the binomial distribution, which is characterized by a single parameter $p$.\n",
    "When we extend things to $d$ binary features and $K$ classes we have for class $k$ parameters $p_{1k},\\ldots,p_{dk}$ and\n",
    "\n",
    "$$\n",
    "p(x_j|y_k)=\\space  \\begin{cases} p_{jk} & \\textrm{when} \\space x_j=1 \\newline\n",
    "1- p_{jk} \\space &\\textrm{when} \\space x_j=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which can be expressed as:\n",
    "\n",
    "$$\n",
    "p(x_j|y_k) = p_{jk}^{x_j} (1-p_{jk})^{1-x_j}.\n",
    "$$\n",
    "\n",
    "Overall we then have:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|y_k) = \\prod_{j=1}^d p_{jk}^{x_j} (1-p_{jk})^{1-x_j}.\n",
    "$$\n",
    "\n",
    "We note that the above expression for $p(x_j|y_k)$ can be written as:\n",
    "\n",
    "$$\n",
    "p(x_j|y_k) = p_{jk}{x_j} + (1-p_{jk})(1-x_j).\n",
    "$$\n",
    "\n",
    "#### Implementation note\n",
    "\n",
    "In practical implementations, it's a good idea to compute this product indirectly by working with the logarithms of the probabilities to avoid underflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Email classification\n",
    "\n",
    "Suppose our vocubulary contains three words a, b and c and we use a multivariate Bernoulli model for our emails, with parameters\n",
    "        \n",
    "     +(spam) = (0.5,0.67,0.33)   \n",
    "     -(not spam)=(0.67,0.33,0.33)   \n",
    "\n",
    "These are the parameter values for $p(a|+), p(b|+), p(c|+)$ and $p(a|-), p(b|-), p(c|-)$, respectively.\n",
    "\n",
    "This means ,for example, that the presence of b is twice as likely in the spam class (+), compared with the non-spam class.\n",
    "\n",
    "Suppose you are given an email that contains the words a and b but not c and hence is described by the bit vector $\\mathbf{x}=(1,1,0)$. \n",
    "We obtain likelihoods \n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|+)={0.5 \\times 0.67 \\times 0.33} = 0.11\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|-)={0.67 \\times 0.33 \\times 0.33} = 0.073\n",
    "$$\n",
    "\n",
    "If the two classes are equally probable we obtain that $\\mathbf{x}$ should be classified as spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deriving the training procedure for Naive Bayes for a Bernoulli distribution we need to discuss the principle of maximum likelihood.\n",
    "\n",
    "### Digression: maximum likelihood\n",
    "\n",
    "Goal:  Fit a probabilistic model $p(x | \\theta)$ to data, i.e.\n",
    "estimate its parameters $\\theta$.  Note that $x$ is a scalar value.\n",
    "\n",
    "We are given independent identically distributed (i.i.d.) data $X = (x_1, x_2,\\ldots, x_N)$.\n",
    "\n",
    "The **Likelihood** of the data:\n",
    "\n",
    "$$\n",
    "  p(X|\\theta)=p(x_1|\\theta)p(x_2|\\theta),...,p(x_N|\\theta)      \n",
    "$$\n",
    "\n",
    "For simplicity we will work with the **Log likelihood**:\n",
    "\n",
    "$$\n",
    "  \\log p(X|\\theta)=\\sum_{i=1}^{N}\\log p(x_i|\\theta)      \n",
    "$$\n",
    "\n",
    "\n",
    "The **Maximum likelihood solution** consists of the parameters $\\theta$ that maximize $ \\log p(X | \\theta) $.  Note that maximizing the likelihood or the log-likelihood is equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood example\n",
    "\n",
    "**Coin Toss**:\n",
    "Estimate the probability $p$ that a coin lands “Heads” using the result of $N$ coin tosses, $h$ of which resulted in heads.\n",
    "\n",
    "The likelihood of the data: $p(X|\\theta)=p^h(1-p)^{(n-h)}$\n",
    "\n",
    "\n",
    "Log likelihood: $\\log p(X|\\theta) = h \\log p + (n-h) \\log(1-p)$\n",
    "\n",
    "Taking the derivative and setting to 0:\n",
    "\n",
    "$$\n",
    "        \\frac{\\partial \\log p(X|\\theta)}{\\partial p}=\\frac{h}{p}-\\frac{(N-h)}{(1-p)}=0\n",
    "$$        \n",
    "\n",
    "$$\n",
    "\\implies p=\\frac{h}{N}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes training\n",
    "\n",
    "Training data: $(N \\times d)$ feature matrix $X$,  and labels $y_1,\\ldots,y_N$.\n",
    "\n",
    "The maximum likelihood estimates for a feature vector $\\mathbf{x}$ and label $y$ are given as follows.\n",
    "\n",
    "The class prior is estimated as the number of examples with a given label:\n",
    "\n",
    "$$\n",
    "\\hat{p}(y)=\\frac{|\\{i:y_i=y\\}|}{N}.\n",
    "$$\n",
    "\n",
    "The likelihood is estimated by the appropriate frequency estimates from the training data matrix:\n",
    "\n",
    "$$\n",
    "\\hat{p}(x_j|y)=\\frac{\\hat{p}(x_j,y)}{\\hat{p}(y)}=\\frac{\\frac{|\\{i:X_{ij}=x_{j},y_{i}=y\\}|}{N}}{\\frac{|\\{i:y_i=y\\}|}{N}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email classification: training the model\n",
    "\n",
    "Suppose we have the following set of emails:\n",
    "\n",
    "Email|a|b|c|Class\n",
    "---|---|---|---|---\n",
    "$e_1$|0|1|0|+\n",
    "$e_2$|0|1|1|+\n",
    "$e_3$|1|0|0|+\n",
    "$e_4$|1|1|0|+\n",
    "$e_5$|1|1|0|-\n",
    "$e_6$|1|0|1|-\n",
    "$e_7$|1|0|0|-\n",
    "$e_8$|0|0|0|-\n",
    "\n",
    "From this table we can compute estimates required for using the model:\n",
    "\n",
    "$\\hat{p}(+)=0.5, \\hat{p}(-)=0.5$\n",
    "\n",
    "$\\hat{p}(a|+)=0.5, \\hat{p}(a|-)=0.75$\n",
    "\n",
    "$\\hat{p}(b|+)=0.75, \\hat{p}(b|-)=0.25$\n",
    "\n",
    "$\\hat{p}(c|+)=0.25, \\hat{p}(c|-)=0.25$\n",
    "\n",
    "The following piece of code computes these probablities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  [0 1]\n",
      "prior: [0.5 0.5]\n",
      "probabilities: \n",
      "y=0 [0.75 0.25 0.25]\n",
      "y=1 [0.5  0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_probabilities(X, y):\n",
    "    N,d = X.shape\n",
    "    labels = np.unique(y)\n",
    "    print(\"labels: \", labels)\n",
    "    # compute the prior\n",
    "    prior=np.zeros(shape=(2,))\n",
    "    for label in labels :\n",
    "        prior[label] = np.sum(y==label)/N\n",
    "    probs=np.zeros(shape=(len(labels), d))\n",
    "    for label in labels:\n",
    "        probs[label] = (np.sum(X[y==label], axis=0) / N) / p[label]\n",
    "\n",
    "    return prior,probs\n",
    "        \n",
    "X = np.array(\n",
    "    [[0, 1, 0],\n",
    "     [0, 1, 1],\n",
    "     [1, 0, 0],\n",
    "     [1, 1, 0],\n",
    "     [1, 1, 0],\n",
    "     [1, 0, 1],\n",
    "     [1, 0, 0],\n",
    "     [0, 0, 0]])\n",
    "y=np.array([1,1,1,1,0,0,0,0])\n",
    "prior,probs = calculate_probabilities(X,y)\n",
    "\n",
    "print('prior:', prior)\n",
    "print('probabilities: ')\n",
    "print('y=0', probs[0])\n",
    "print('y=1', probs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09375 0.5625 ]\n",
      " [0.03125 0.1875 ]\n",
      " [0.84375 0.1875 ]\n",
      " [0.28125 0.5625 ]\n",
      " [0.28125 0.5625 ]\n",
      " [0.28125 0.0625 ]\n",
      " [0.84375 0.1875 ]\n",
      " [0.28125 0.1875 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09375, 0.5625)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(probs, prior, X):\n",
    "    N,d = X.shape\n",
    "    pred = np.zeros((N,len(prior)))\n",
    "    for i in range(N):\n",
    "        for label in range(len(prior)) :\n",
    "            p = 1.0\n",
    "            for j in range(d):\n",
    "                p = p * probs[label][j]**X[i][j] * (1-probs[label][j])**(1-X[i][j])\n",
    "            pred[i][label]=p / prior[label]\n",
    "    return pred\n",
    "\n",
    "print(predict(probs, prior, X))\n",
    "0.25*0.25*0.75/0.5,0.5*0.75*0.75/0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False, False,  True,  True,  True])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = predict(probs, prior, X)\n",
    "p[:,0] > p[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on Naive Bayes\n",
    "\n",
    "Usually features are not conditionally independent.\n",
    "Despite that, Naive Bayes is still one of the most widely used classifiers due to its simple implementation and fast training.\n",
    "It often performs well even when the assumption is violated.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When there are few training examples\n",
    "\n",
    "What if you have never seen a training example where a is present within our positive examples ($y=+$)?  In that case we have that:\n",
    "\n",
    "$p(\\mathbf{x} | +) = p(a | +) p(b | +) p(c | +) = 0$\n",
    "\n",
    "What to do?\n",
    "\n",
    "Add \"virtual examples\" for which a is present when y=spam.  In other words, rather than initialize all counts in the calculation of $p(x_j|y)$ to be zero, initialize them to some small numbers.\n",
    "These are called **pseudo-counts**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Naive Bayes in scikit-learn with the 20 newsgroups dataset\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split into two subsets: one for training (or development) and the other one for testing (or for performance evaluation).\n",
    "\n",
    "For illustration purposes we will use only two categories out of the 20: `alt.aetheism` and `sci.space`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "cats=['alt.atheism', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'), \n",
    "                                      categories=cats)\n",
    "# we eliminated the headers and footers of the emails which \n",
    "# are a giveaway of the class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 1 1]\n",
      "(1073,)\n",
      "['alt.atheism', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.target)\n",
    "print(newsgroups_train.target.shape)\n",
    "print(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': \\n: >> Please enlighten me.  How is omnipotence contradictory?\\n: \\n: >By definition, all that can occur in the universe is governed by the rules\\n: >of nature. Thus god cannot break them. Anything that god does must be allowed\\n: >in the rules somewhere. Therefore, omnipotence CANNOT exist! It contradicts\\n: >the rules of nature.\\n: \\n: Obviously, an omnipotent god can change the rules.\\n\\nWhen you say, \"By definition\", what exactly is being defined;\\ncertainly not omnipotence. You seem to be saying that the \"rules of\\nnature\" are pre-existant somehow, that they not only define nature but\\nactually cause it. If that\\'s what you mean I\\'d like to hear your\\nfurther thoughts on the question.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert the text into vectors of numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1073, 18217)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the test data\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'), \n",
    "                                     categories=cats)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to apply the scikit-learn version of Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn import metrics\n",
    "#nb = MultinomialNB(alpha=.01)\n",
    "nb = BernoulliNB(alpha=0.1)\n",
    "nb.fit(X_train, newsgroups_train.target)\n",
    "\n",
    "pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the parameter $\\alpha$ of the scikit-learn implementation is a hyperparameter that controls the use of pseudo-counts, i.e. what value to assign a feature that has not been seen.  Like the $\\alpha$ parameter of linear regression and logistic regression, it serves to help avoid overfitting.\n",
    "\n",
    "The accuracy of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8597475455820477\n"
     ]
    }
   ],
   "source": [
    "print ('accuracy: ', np.sum(pred==newsgroups_test.target) / len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'sci.space']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[310,   9],\n",
       "       [ 91, 303]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(newsgroups_test.target, pred,  labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1073, 593)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.target),np.sum(newsgroups_train.target==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot = plot_confusion_matrix(nb, X_test, newsgroups_test.target, \n",
    "                             display_labels=newsgroups_train.target_names,\n",
    "                             values_format = 'd',\n",
    "                             cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our model to sentences that seem relevant to to the two categories ('alt.atheism', 'sci.space') and see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending a payload to the international space station\n",
      "predicted class:  sci.space\n",
      "probabilities:  [0.07910088 0.92089912]\n",
      "religion and its role in American politics\n",
      "predicted class:  alt.atheism\n",
      "probabilities:  [9.99999874e-01 1.26343962e-07]\n",
      "the astronaut launched yesterday has no spiritual beliefs\n",
      "predicted class:  alt.atheism\n",
      "probabilities:  [9.99832216e-01 1.67783690e-04]\n",
      "the astronaut reached the station\n",
      "predicted class:  alt.atheism\n",
      "probabilities:  [0.97397014 0.02602986]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'sending a payload to the international space station',\n",
    "    'religion and its role in American politics',\n",
    "    'the astronaut launched yesterday has no spiritual beliefs',\n",
    "    'the astronaut reached the station',\n",
    "]\n",
    "X_unlabeled = vectorizer.transform(sentences)\n",
    "pred = nb.predict(X_unlabeled)\n",
    "probs = nb.predict_proba(X_unlabeled)\n",
    "for i in range(len(sentences)) :\n",
    "    print (sentences[i])\n",
    "    print('predicted class: ', newsgroups_train.target_names[pred[i]])\n",
    "    print('probabilities: ', probs[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "Up until now we assumed that each feature was either present or absent.  In probabilistic terms, we modeled each feature using the binomial distribution.  For text data for example, there is potential information in the counts, i.e. how many times a word appears in a document, not just its presence of absence.  To model counts we can use the multinomial distribution:  each feature is now associated with a multinomial variable.\n",
    "\n",
    "\n",
    "\n",
    "### Exercises\n",
    "\n",
    "* The `CountVectorizer` converts all tokens to lowercase before generating its output.  Why is that useful?\n",
    "\n",
    "* Use [multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) instead of Bernoulli Naive Bayes.  You will need to recreate the training/test vectors to be counts rather than presence/absence.  Which version of Naive Bayes performed better?  Can you provide an explanation why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial naive Bayes applied to the 20 newsgroups data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian naive Bayes\n",
    "\n",
    "If you are dealing with data whose features are continuous rather than discrete, you need to model $p(x_i | y)$ using an appropriate distribution.  A common choice is to use the normal (aka Gaussian) distribution:\n",
    "\n",
    "$$\n",
    "p(x_i | y) = \\frac{1}{\\sqrt{2\\pi \\sigma_{iy}^2}} e^{- \\frac{(x_i - \\mu_{iy})^2}{2\\sigma_{iy}^2}},\n",
    "$$\n",
    "\n",
    "where $\\mu_{iy}$ is the mean for feature $i$ in class $y$ and $\\sigma_{iy}$ is the standard deviation for feature $i$ in class $y$.\n",
    "\n",
    "In scikit-learn Gaussian naive Bayes is implemented as the class [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB).\n",
    "\n",
    "### Exercise\n",
    "\n",
    "* Apply Gaussian Naive Bayes to the iris dataset, and compare its performance to the classifier of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "An advantage of naive Bayes is that it requires a relatively small number of training data to estimate its parameters, so for relatively small datasets it may work very well.\n",
    "For larger datasets other classifiers will likely outperform it (see \n",
    "Caruana et al. referenced below).\n",
    "In any case, it is still a good baseline, especially for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Domingos, P., & Pazzani, M. (1997). Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier. Machine Learning. 29, 103-130.\n",
    "* Caruana, R.; Niculescu-Mizil, A. (2006). An empirical comparison of supervised learning algorithms. Proc. 23rd International Conference on Machine Learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
