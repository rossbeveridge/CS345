{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is part of  course materials for CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
    "Original versions were created by Asa Ben-Hur.\n",
    "The content is availabe [on GitHub](https://github.com/asabenhur/CS345).*\n",
    "\n",
    "*The text is released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/), and code is released under the [MIT license](https://opensource.org/licenses/MIT).*\n",
    "\n",
    "<img style=\"padding: 10px; float:right;\" alt=\"CC-BY-SA icon.svg in public domain\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/d0/CC-BY-SA_icon.svg\" width=\"125\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github//asabenhur/CS345/blob/master/notebooks/module05_03_model_selection.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using nested cross-validation\n",
    "\n",
    "This notebook covers the topic of selecting classifier parameters using nested cross-validation cross validation.\n",
    "\n",
    "As discussed in an earlier notebook, whenever a classifier has hyperparameters that require setting, the appropriate protocol for evaluating classifier accuracy is as follows:\n",
    "\n",
    "* Divide the data into separate train, validation, and test sets\n",
    "* Loop over all combinations of values of hyperparameters\n",
    "* For each value combination, train a model on the training set and evaluate it on the validation set\n",
    "* Choose best performing set of parameters and retrain the classifier using the training and validation set.\n",
    "* Evaluate the resulting classifier on the test set\n",
    "\n",
    "Now that we know how to perform cross-validation, we can update this protocol to make use of this technique for more efficient use of our data.\n",
    "\n",
    "### Model selection using grid search in scikit-learn\n",
    "\n",
    "Before we describe the procedure for model selection using cross-validation, we will introduce scikit-learn functionality to perform model selection using grid-search, which will simplify the code for performing this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# you will get better results if you standardize the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do plain cross-validation, without trying to choose a particular value for the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.931516845210371"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "accuracy = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This used the default parameter values chosen by the developers of scikit-learn.  The defaults are (usually) quite good, but each classifier has parameters that require tuning for optimal performance on a given problem.  **Typically there is no parameter choice that would be universally good** (among those parameters that have a strong effect on classifier performance).\n",
    "So let's see if we can do better if we choose parameters optimized for the given dataset rather than using the defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# for the nearest neighbor classifier we can select multiple \n",
    "# hyperparameters:\n",
    "# n_neighbors - number of neighbors to base classification on\n",
    "# p - whether to use the L1 or L2 norm when computing distances\n",
    "# weights - the sklearn KNN implementation provides the option to\n",
    "# weight the contribution of each example uniformly or inversely \n",
    "# proportional to its distance\n",
    "# this leads to the following grid of values:\n",
    "\n",
    "param_grid = {'n_neighbors': [3,5,7,9,11,13],\n",
    "              'p': [1, 2]}\n",
    "\n",
    "classifier = GridSearchCV(KNeighborsClassifier(), param_grid)\n",
    "\n",
    "classifier.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you `fit` a `GridSearchCV` runs the following grid-search procedure:\n",
    "\n",
    "**Training using grid-search**:\n",
    "\n",
    "**Input**:  a classifier and a set of parameters with a set of values to try for each parameter.\n",
    "\n",
    "* Loop over all combinations of classifier parameters.\n",
    "* For each value of the classifier parameters, perform cross-validation on the training data and evaluate accuracy\n",
    "* Use the best scoring model parameters, and retrain on the entire dataset \n",
    "\n",
    "In the example above, grid search over the parameters `n_neighbors` and `p` translates into the following nested for loops for creating all combinations of values:\n",
    "\n",
    "```python\n",
    "for n_neighbors in n_neighbors_list :\n",
    "    for p in p_list :\n",
    "        # evaluate classifier using cross-validation \n",
    "        # using p and n_neighbors\n",
    "```\n",
    "\n",
    "The trained `GridSearchCV` object contains information regarding the best parameter values and detailed cross-validation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 9, 'p': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_n_neighbors', 'param_p', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])\n",
      "[0.92793045 0.91914299 0.93145474 0.92794597 0.93148579 0.92617606\n",
      " 0.93851886 0.93147027 0.93674895 0.92970036 0.93324018 0.93324018]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.cv_results_.keys())\n",
    "print(classifier.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ``fit`` method of the ``GridSearchCV`` classifier performed cross-validation to find the best parameters.  This is sometimes called external cross-validation. We can now easily performed **nested cross-validation** simply by performing cross-validation over the ``GridSearchCV`` classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9350256171401956"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = cross_val_score(classifier, X, y, cv=cv, \n",
    "                           scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our original procedure which uses train/validation/test set splits is shown here:\n",
    "\n",
    "* Divide the data into separate train, validation, and test sets\n",
    "* Loop over all combinations of values of hyperparameters\n",
    "* For each value combination, train a model on the training set and evaluate it on the validation set\n",
    "* Choose best performing set of parameters and retrain the classifier using the training and validation set.\n",
    "* Evaluate the resulting classifier on the test set\n",
    "\n",
    "It is replaced with the following version which uses cross-validation instead:\n",
    "\n",
    "* Divide the data into $k$ subsets\n",
    "* Loop over the $k$ subset, and at each iteration one of them serves as a test set.\n",
    "  * Loop over all combinations of values of the hyperparameters\n",
    "  * For each value combination, evaluate the classifier using cross-validation\n",
    "  * Choose best performing set of parameters and re-train the classifier using all the training data for that fold.\n",
    "  * Evaluate the resulting classifier on the test set for the given fold\n",
    "* Combine accuracy estimates over all the folds.\n",
    "\n",
    "\n",
    "<img style=\"padding: 10px; float:center;\" alt=\"nested cross validation; created by Asa Ben-Hur inspired by figure from https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch06/ch06.ipynb\" src=\"https://github.com/asabenhur/CS345/raw/master/notebooks/figures/nested_cross_validation.jpg\" width=\"600\">\n",
    "\n",
    "This process looks quite expensive.  Let's be more concrete and evaluate how many models we need to train when considering say 100 combinations of hyperparameter values and performing 5-fold cross-validation in both the inner and outer loops.\n",
    "For fitting the grid-search object: for each combination of hyperparameter values we run 5-fold cross-validation, resulting in training 500 models.\n",
    "If we are doing nested cross validation the overall number of models is $100 \\times 5 \\times 5 = 2,500$.\n",
    "The process of cross-validation and nested cross-validation is highly parallelizable since folds can be trained and evaluated independently of each other for each combination of values of the hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* Perform model selection for decision trees to select the optimal value of the `max_depth` and `min_samples_split` hyperparameters using scikit-learn's [decision tree implementation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). The `min_samples_split` is the minimum number of samples required to split an internal node in the tree. For each parameter choose a wide enough range of values that makes sense.  Compare the accuracy with and without model selection.  Accuracy comparison should be performed using cross-validation.  What optimal values of the parameters were chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "If a full grid search is too expensive, you can choose to perform randomized grid search, which randomly chooses points from the grid of values.  This is implemented in scikit-learn as the class [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV).  In neural networks, which have a very large number of parameters a complete grid search is not feasible, and randomized grid search is the only viable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining unbiased results by preventing information leak between train and test data: an example using feature selection\n",
    "\n",
    "When selecting one or two hyperparameters, the bias in accuracy when comparing correct experimental protocol with a protocol that allows for information leakage between the train and test sets can be quite small.\n",
    "However, in situations where a large number of hyper-parameters needs to be selected the difference can be dramatic.  Consider as an example the problem of feature selection.  In this problem we can think of each feature as being associated with a hyper-parameter that controls whether that feature should be included.\n",
    "\n",
    "We will demonstrate this using a dataset describing the activity of a large number of genes, from which disease state can be inferred.  The particular dataset we will analyze looks at biological samples taken from leukemia patients with two types of leukemia: acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).  The data was taken from the following publication:\n",
    "\n",
    "* Golub, Todd R., et al. \"Molecular classification of cancer: class discovery and class prediction by gene expression monitoring.\" Science  (1999): 531-537."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72, 7128), (72,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "link = \"https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv\"\n",
    "# retrieve the contents of the file\n",
    "contents = requests.get(link)\n",
    "lines = contents.text.split()\n",
    "# the data is in csv format and the labels appear in the first \n",
    "# row of the dataset\n",
    "class_convert = {'ALL':1, 'AML':0}\n",
    "y = np.array([class_convert[token] for token in lines[0].split(',')])\n",
    "X = np.array([ [float(token) for token in line.split(',')] \n",
    "              for line in lines[1:] ])\n",
    "X = X.transpose()\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's evaluate the accuracy of the scikit-learn perceptron using all the features (note - this is different than the version of the perceptron studied in class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9038095238095238"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "p = Perceptron()\n",
    "accuracy = cross_val_score(p, X, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll perform feature selection on the entire dataset - even though it's the wrong way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9457142857142857"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# feature selection using RFE\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "selector = RFE(Perceptron(), step=0.1, n_features_to_select=20)\n",
    "\n",
    "# perform feature selection on the data\n",
    "X_sel = selector.fit_transform(X, y)\n",
    "print ('number of features: ', X_sel.shape[1])\n",
    "\n",
    "accuracy = cross_val_score(p, X_sel, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed feature selection using a method called **Recursive Feature Elimination** (RFE).  RFE builds on the intuition that the magnitude of the weight vector of a linear classifier is a good indication of a feature's usefulness.\n",
    "Rather than simply removing the features with the lowest value of the weight vector, RFE alternates between training a linear classifier and removing a small number of features with the lowest weight vector magnitude.  RFE was proposed in the following publication:\n",
    "\n",
    "> Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., “Gene selection for cancer classification using support vector machines”, Mach. Learn., 46(1-3), 389–422, 2002.\n",
    "\n",
    "As proposed, it uses support vector machines as the base classifier, but it can be used with any linear classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides us with functionality for easily performing feature selection the correct way, which leads to a very different estimate of accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9190476190476191"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = RFE(Perceptron(), step=0.1, n_features_to_select=20)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [('feature_selector', selector),\n",
    "     ('classifier', Perceptron())])\n",
    "accuracy = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of using 20 features was arbitrary and not optimal as we can see by performing grid search over the number of selected features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9457142857142857"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, \n",
    "                          {'feature_selector__n_features_to_select' : \n",
    "                           [5, 10, 20, 30, 40, 50]})\n",
    "accuracy = cross_val_score(grid_search, X, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing model selection over an attribute of a component of a pipeline requires you to access the name of the attribute in combination with the name your provided for that step in the pipeline.\n",
    "\n",
    "To determine the optimal number of features, use `fit` instead of cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_selector__n_features_to_select': 30}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X, y)\n",
    "grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
